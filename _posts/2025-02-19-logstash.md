---
title: "Logstash 핵심 구성 요소: 파이프라인, 플러그인, 코덱"
description: "입력/필터/출력 파이프라인 설정, 주요 플러그인 기능과 사용법, 데이터 인코딩/디코딩 코덱 활용" 
date: 2025-02-16 23:40:00 +0900
categories: [Database, ETL]
tags: [Logstash, Pipeline, Plugin, Input, Filter, Output, Codec, Configuration]
math: false
mermaid: true
---

## 1. Logstash

Logstash는 플러그인 기반의 오픈소스 데이터 처리 파이프라인 도구이다. 데이터 전처리 과정을 별도의 애플리케이션 작성 없이 간단한 설정으로 수행할 수 있다.

- Logstash 실행
    ``` shell
    ./bin/logstash.bat -e 'input { stdin { } } output { stdout { } }'
    ```
    - -e 옵션을 사용하면 콘솔에서 직접 파이프라인을 설정할 수 있다
- 파이프라인
  - 입력, 필터, 출력 세 가지 구성요소로 이뤄진다.
  - 로그스태시를 실행하기 위해서는 반드시 파이프라인 설정이 필요하다
  - 일반적으로 파이프라인은 따로 설정 파일을 만들어서 기록하거나 config 폴더의 pipelines.yml에 기록
  - 파이프라인 기본 템플릿
    ``` 
    input {
        { 입력 플러그인 }
    }
    
    filter {
        { 필터 플러그인 }
    }
    
    output {
        { 출력 플러그인 }
    }
    ```

<br>
<br>

## 2. 입력
- 자주 사용하는 입력 플러그인

|plugin|description|
|---|---|
|file|리눅스의 tail -f 명령처럼 파일을 스트리밍하여 이벤트를 읽어들인다|
|syslog|네트워크를 통해 전달되는 시스로그를 수신|
|kafka|카프카의 토픽에서 데이터를 읽어들인다|
|jdbc|JDBC 드라이버로 지정한 일정마다 쿼리를 실행해 결과를 읽어들인다|

- 파이프라인 설정 파일 작성
  - 파이프라인 설정 파일의 위치 및 확장자가 크게 중요하지는 않음
  - 파일 플러그인 추가
    ```
    input {
        file {
            path => "C:/elasticsearch-7.10.1/logs/elasticsearch.log"
            start_position => "beginning"
        }
    }
        
    output {
        stdout { }
    }
    ```
    - `path`: 읽어 들일 파일 위치를 결정한다
      - 파일에 로그가 쌓이면 실시간으로 elasticsearch.log 파일의 변경을 감지해 읽어들인다.
    - `start_position`: 최초 파일을 발견했을 때 파일을 읽을 위치로, 파일의 시작 부분부터 읽어들일지(begining) 끝부분부터 새로운 라인만 읽어들일지(end) 정할 수 있다
- 파이프라인 설정 파일 지정
    ``` shell
    ./bin/logstash.bat -f ./config/logstash-test.conf
    ```
  - -f 옵션은 설정 파일이나 폴더를 지정하는 옵션으로 방금 만들었던 logstash-test.conf를 파이프라인 설정에 사용하게 된다

<br>
<br>

## 3. 필터
- 필터는 입력 플러그인이 받은 데이터를 의미 있는 데이터로 구조화하는 역할을 한다
- 비츠나 키바나 등에서 입력받은 데이터를 로그스태시 필터를 이용해 필요한 정보만 손쉽게 추출하거나 형태를 변환하고 부족한 정보는 추가하는 등 전반적인 데이터 정제/가공 작업을 수행할 수 있다
- 정형화된 데이터는 엘라스틱서치나 아마존 S3 같은 스토리지에 전송되어 분석 등의 용도로 활용된다
- 자주 사용하는 필터 플러그인

    |plugin|description|
    |---|---|
    |grok|gork 패턴을 사용해 메시지를 구조화된 형태로 분석한다. grok 패턴은 일반적인 정규식과 유사하나, 추가적으로 미리 정의된 패턴이나 필드 이름 설정, 데이터 타입 정의 등을 도와준다.|
    |dissect|간단한 패턴을 사용해 메시지를 구조화된 형태로 분석한다. 정규식을 사용하지 않아 grok에 비해 자유도는 조금 떨어지지만 더 빠른 처리가 가능하다|
    |mutate|필드명을 변경하거나 문자열 처리 등 일반적인 가공 함수들을 제공한다|
    |date|문자열을 지정한 패턴의 날짜형으로 분석한다|

  - dissect와 grok 플러그인은 패턴을 이용해 구문 분석을 한다는 공통점이 있지만 성능 차이가 있다. 로그 형식이 일정하고 패턴이 변하지 않는다면 dissect를 사용하는 것이 좋다. 패턴을 해석하지 않아 속도가 빠르기 때문이다. 하지만 로그 형태가 일정하고 장담하기 힘들다면 예외 처리나 패턴이 자유로운 grok을 사용하는 것이 좋다.
  - dissect로 분석이 가능한 문자열은 무조건 dissect로 분석하는 것이 성능상 좋으며, grok는 dissect와 기타 필터 조합으로 간단하게 해결되지 않는 정규 표현식이 필요한 경우에만 사용하는 것이 좋다

- sincedb 데이터베이스 파일
  - sincedb 데이터베이스 파일은 파일을 어디까지 읽었는지 기록하는 파일이다. (파일이 마지막으로 읽힌 오프셋값이 기록됨)
  - start_position은 파일을 불러들이는 최초 한 번만 적용
  - 이후부터는 sincedb 데이터베이스 파일이 있는지 보고, 있다면 sincedb 데이터베이스 파일에 기록되어 있는 위치부터 파일을 읽는다
  - 아래처럼 `sincedb_path ⇒ “nul”`로 지정하면 sincedb 파일을 만들지 않기 때문에 이전 파일을 읽었던 기록이 없어서 매번 로그스태시를 실행할 때마다 파일을 처음부터 읽게 된다

    ```
    input {
        file {
            path => "C:/elasticsearch-7.10.1/logs/elasticsearch.log"
            start_position => "beginning"
            sincedb_path => "nul"
        }
    }
    ```

<br>
<br>

## 4. 출력
- Logstash는 JSON 형태로 데이터를 출력
- @ 기호는 사용자가 만든 필드와 충돌을 방지하기 위해 붙음
  - @가 붙은 필드는 로그스태시에 의해 생성된 정보, 붙지 않은 필드는 수집을 통해 얻어진 정보
- 자주 사용하는 출력 플러그인

    |plugin|description|
    |---|---|
    |elasticearch|가장 많이 사용되는 출력 플러그인으로, bulk API를 사용해 엘라스틱서치에 인덱싱을 수행한다|
    |file|지정한 파일의 새로운 줄에 데이터를 기록한다|
    |kafka|카프카 토픽에 데이터를 기록한다|

- file과 elasticsearch 출력 플러그인 사용 예시

    ```
    output {
        file {
            path => "C:/logstatsh-7.10.1/config/output.json"
        }
        elasticsearch {
            index => "output"
        }
    }
    ```
    - 파일 플러그인은 데이터를 파일 형식으로 전송한다. path 옵션에 저장할 위치를 적어주면 된다
    - 엘라스틱서치 플러그인은 데이터를 엘라스틱서치에 전송한다. 인덱스명을 설정할 수 있고 호스트 URL이나 라우팅 같은 다양한 옵션을 사용할 수 있다

- 엘라스틱서치 플러그인 옵션

    |option| description                             |
    |---|---|
    |hosts|이벤트를 전송할 엘라스틱서치의 주소|
    |index|이벤트를 인덱싱할 대상 인덱스|
    |document_id|인덱싱될 문서의 아이디를 직접 지정할 수 있는 옵션|
    |user/password|엘라스틱서치에 보안 기능이 활성화되어 있을 때 인증을 위한 사용자 이름과 비밀번호|
    |pipeline|엘라스틱서치에 등록된 인제스트 파이프라인을 활용하기 위한 옵션|
    |template|커스텀 인덱스 템플릿 파일 경로 지정|
    |template_name|커스텀 인덱스 파일을 엘라스틱서치에 어떤 이름으로 등록할지를 설정|

<br>
<br>

## 5. 코덱
- 코덱은 입력과 출력 단계에서 데이터의 인코딩/디코딩을 담당
- 입력/출력에만 코덱을 사용하고 필터에는 코덱을 사용할 수 없다
- 자주 사용하는 코덱 플러그인

    |plugin|description|
    |---|---|
    |json|입력 시 JSON 형태의 메시지를 객체로 읽어들인다. 출력 시에는 이벤트 객체를 다시금 JSON 형태로 변환한다|
    |plain|메시지를 단순 문자열로 읽어들인다. 출력 시에는 원하는 포맷을 지정할 수 있다|
    |rubydebug|로그스태시의 설정을 테스트하거나 예기치 못한 파이프라인 설정 오류를 디버깅하기 위한 목적으로 주로 사용되며, 출력 시 Ruby의 해시 형태로 이벤트를 기록. 입력 시에는 사용되지 않는다|

<br>
<br>

## 6. 다중 파이프라인
- 다중 파이프라인은 하나의 로그스태시에서 여러 개의 파이프라인을 독립적으로 실행할 수 있게 한다
- 다중 파이프라인 설정
  - pipelines.yml

    ``` yml
    - pipeline.id: mypipe1
      path.config: "/logstash-7.10.1/config/mypipe1.conf"
    - pipeline.id: mypipe2
      path.config: "/logstash-7.10.1/config/mypipe2.conf"
    ```
    